%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt]{article}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{array}
\usepackage{amssymb}
\usepackage[unicode=true]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage{url}
\usepackage{fullpage}
\usepackage{comment}
\usepackage{fancyhdr}
\usepackage{lastpage}


%\pagestyle{empty}


\fancyhf{}
\lfoot{Michael Betancourt Biographical Sketch}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{1pt}

\makeatother

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                                 Header                                                      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
\textbf{Biographical Sketch}
\par\end{center}

\noindent \rule{6.75in}{0.25mm} NAME: Benjamin Goodrich

\noindent \rule{6.75in}{0.25mm} eRA COMMONS USER NAME: B\_GOODRICH

\noindent \rule{6.75in}{0.25mm} POSITION TITLE: Lecturer in Discipline
in Political Science at Columbia University

\noindent \rule{6.75in}{0.25mm} EDUCATION/TRAINING \vspace{3mm}


\noindent %
\begin{tabular*}{6.5in}{@{\extracolsep{\fill}}lll>{\raggedright}p{1.5in}}
INSTITUTION AND LOCATION  & DEGREE  & COMPLETION  & FIELD\tabularnewline
 &  & DATE  & OF STUDY \tabularnewline
Emory University, Atlanta, GA & B.A.  & 05/2001 & Political Science\tabularnewline
Emory University, Atlanta, GA & M.A.  & 05/2001 & Political Science\tabularnewline
Harvard University, Cambridga, MA & Ph.D. & 05/2010 & Government and Social Policy\tabularnewline
Columbia University, New York, NY  & Postdoctoral  & 06/2013  & Applied Statistics \tabularnewline
\end{tabular*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                         Personal Statement                                           %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{A. Personal Statement}

Although my degrees have been in political science, I am essentially
a computational statistician with interests in multiple applied fields. 
I will be a co-principal investigator on this grant in order to build 
and enhance accessible software to facilitate complex hierarchical 
modeling by clinical outcomes researchers.

My co-principal investigator, Michael Andreae is a perioperative physician; 
He and his colleagues at the Albert Einstein College of Medicine
have extensive experience in clinical medicine and applied statistical modeling 
for outcomes research, which will allow me to focus on the complimentary
tasks of adding to our existing software (\textit{Stan}, \textit{rstan}, 
\textit{rstanarm}, \textit{shinystan}, and \textit{loo}) for hierarchical modeling 
In addition, I will be heavily involved in teaching and dissemination through 
workshops and online user groups. But all of our software has been developed 
jointly, and my previous experience with the NIH has been as part of a team of
researchers, so I anticipate no problems leading the project anlong with 
Michael and collaporating frequently with the senior and junior co-investigators.

I have been a core developer of the Bayesian statistical software project called 
\textit{Stan} almost since its inception in 2011 and am the maintainer of two packages 
for the statistical software environment R in which we implement our new software 
\textit{rstan} and \textit{rstanarm}. The \textit{rstan} package provides an interface 
to the Stan library from the R environment, while \textit{rstanarm} is a narrower 
interface to a few models that are written in the Stan language. In addition, from 2010 to
2013 I developed \textit{mi}, which is an R package for multiple imputation of missing data 
as a postdoctoral researcher on an Institute for Education Sciences grant.

This grant application proposes substantial enhancements to our aforementioned 
software projects, which have been developed in collaboration with Andrew Gelman and 
other researchers at and outside Columbia University. \textit{Stan's} selling point is a 
Hamiltonian Monte Carlo algorithm that tends to achieve a much higher effective sampling size
than any other Bayesian software. \textit{Stan} and the interfaces to Stan have been 
well-received in a variety of scientific fields. In the past five years, \textit{Stan} 
has already become prominent in applied statistics and the social sciences, and we 
would like to bring these achievements to the field of medicine and expand upon 
them during the next five years. 

As a lecturer at Columbia University, I teach both Bayesian hierarchical modeling 
and multiple imputation in the Quantitative Methods in the Social Sciences master's
program. Between semesters, I give workshops to promote and disseminate \textit{Stan}, 
such as a two-day workshop on \textit{Stan} at a firm that employs dozens of data 
scientists who study agriculture in light of climate change. This experience has 
prepared to me to further disseminate our new software to medical researchers in workshops 
and online.

I am personally thrilled about this project not only because of the interdisciplinary 
collaboration with applied clinical data scientists to enhance our advanced algorithms 
and models for data-driven clinical outcomes research. 

\vspace{2mm}


\noindent Bob Carpenter et al., ``Stan: A Probabilistic Programming
Language'', (\textit{accepted to the Journal of Statistical Software})

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                        Positions and Honors                                         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{B. Positions and Honors}

2010 \textendash{} 2013, Postdoc, Applied Statistics Center, Columbia
University, New York, NY

\noindent 2013 \textendash{} present, Lecturer in Discipline in Political
Science, Columbia University, New York, NY

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                       Contribution to Science                                       %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{C. Contribution to Science}


\subsection*{Stan: A probabilistic programming language}

I have been a core developer of Stan since 2011, which is a suite
of software that enables scientists to analyze their data and make
Bayesian inferences. It is difficult to estimate how many people use
Stan because it is available to download for free on a variety of
sites around the internet that do not directly count the number of
unique users. We do know that a recent version of the Stan user manual
has been downloaded over 5000 times and that the Python interface
to Stan has been downloaded 6000 times in the past month. The R interface
to Stan, which I am the maintainer of, is more popular than the Python
interface and probably all other Stan interfaces combined.

My contribution to the Stan project has primarily been coding for
multivariate probability distributions, maintaining the R packages
that provide interfaces to Stan, and answering questions on the Stan-users
email list, which has over 1500 members. I have participated in more
than 660 threads on Stan-users and an additional 700 threads on the
email list for Stan developers. Multivariate statistics is much more
challenging than univariate statistics, but Stan has done more than
almost any other software to provide attractive options for scientists
who need to utilize multivariate probability distributions, which
arise when modeling multiple outcome variables simultaneously, when
employing hierarchical models, and other research contexts. In particular,
Stan has done more to popularize modeling with correlation matrices
than have the original papers whose ideas were incorporated into Stan
and extended upon by me personally.

Arguably, it is the data scientists using Stan who make novel contributions 
to science. While that is true, Stan can be and has been a necessary
tool which facilitates that process. Most Stan users initially turn to Stan because 
their research problem is too complicated to be handled well by more commonly available 
open source and commercial software. Thus, Stan provides the means for scientists to 
make their well-founded inferences from data.

\vspace{2mm}


\noindent Bob Carpenter et al., ``Stan: A Probabilistic Programming
Language'', (\textit{accepted to the Journal of Statistical Software})


\subsection*{rstanarm: An R package to democratize Stan}

Part of the power of Stan is that researchers can specify almost any
model with the Stan language, and Stan's algorithms will likely be able to 
efficiently produce a set of random draws from the posterior distribution of that model, 
perhaps after some transformations, reparameterizations and other tweaks. However, the 
hurdle of specifying a model in the Stan language can be too high for beginners or for 
quantitative researchers that have limited formal training in probability theory. 
The rstanarm R package is our latest attempt to make Stan accessible to a wider 
set of researchers by providing a core set of models in the Stan language that 
are ready to be executed by R functions whose syntax is very familiar to 
anyone who has fit models in R before.

For example, the rstanarm package currently includes Bayesian versions
of linear regression and ANOVA models, generalized linear models with
a variety of likelihood and link functions, so-called generalized
''mixed'' models possibly with smooth additive terms such as splines,
and models for ordinal outcomes. Within two days of rstanarm's first
release, an R enthusiast named Wayne Folta (who is not part of the
Stan development team and was until that time unkown to us) 
\href{http://thinkinator.com/2016/01/12/r-users-will-now-inevitably-become-bayesians/}{blogged}
that the ''third reason {[}why everyone isn\textquoteright t using
Bayesian methods for regression modeling{]} has recently been shattered
in the R world by not one but two packages: brms and rstanarm. Interestingly,
both of these packages are elegant front ends to Stan''. Folta summarized
by saying 
\begin{quote}
Bayesian modeling is a general machine that can model any kind of
regression you can think of. Until recently, if you wanted to take
advantage of this general machinery, you\textquoteright d have to
learn a general tool and its language. If you simply wanted to use
Bayesian methods, you were often forced to use very-specialized functions
that weren't flexible. With the advent of brms and rstanarm, R users
can now use extremely flexible functions from within the familiar
and powerful R framework. Perhaps we won't all become Bayesians now,
but we now have significantly fewer excuses for not doing so. This
is very exciting!
\end{quote}
In the future, we would like to add to the collection of models in
rstanarm and include them in other interfaces to Stan besides R. In
particular, we would like to add functionality that is essential to
medical researchers that Michael Andreae and Charles Hall have
suggested.

\vspace{2mm}


Gabry, Jonah and Ben Goodrich. ``How to Use the rstanarm Package.''
R package \href{https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html}{vignette}
(2015).


\subsection*{mi: An R package for multiple imputation of missing data}

Missing and incomplete date are a major problem in survey research 
and electronic health records alike. In a medical
context, some data may be missing because they were not collected. For example, 
in the absence of pertinent symptoms, 
a doctor is less likely to order an expensive or invasive 
test that would not be expected to add any relevant information.
Conversely, symptomatic patients  are more likely to be tested. 
As a result, data are not missing completely at random, data are 
collected or missing contingent on disease status. 
Thus, an analysis limited to only the complete cases will produce severely biased
estimates for the population as a whole.

The same problem manifests itself in different ways in education research and the
social sciences where some data is missing because the survey respondents
(or other units of observation) refuse to provide it. Missing data
is very common for sensitive questions like income and race. 
Again, any analysis of only the complete cases is known to produce biased results.

In recent years, the most common pro-active approach to dealing with
missing data is to impute the missing values multiple times using
some sort of model. There are two main algorithms for modeling variables with
missing data, one of which is implemented by my \textit{mi} R package, which has
been in development for about a decade, was rewritten along with collaborators
as part of my postdoctoral research at Columbia University between
2010 and 2013, and is still being maintained by me. The \textit{mi} package
and other implementations of similar imputation algorithms have helped
a generation of scientists handle incomplete data in a more principled
fashion. My fellow developers of the \textit{mi} package and I have recently
published a paper in the leading journal for political methodology comparing its 
performance to the other main algorithm for modeling incomplete data. We found that 
\textit{mi}'s approach is preferable for discrete variables with missingness, which 
are the vast majority of the variables collected in the social sciences and in clinical 
outcomes research.

For this project, we will integrate \textit{rstanarm} and \textit{mi} by using
the former to estimate the quantities needed by the latter to multiply impute
the missing values. In addition, we have plans to to introduce
a third category of imputation algorithm that builds on the combined
strengths of the two most popular algorithms today.

\vspace{2mm}


Kropko, Jonathan, et al. ``Multiple imputation for continuous and
categorical data: Comparing joint multivariate normal and conditional
approaches.'' \emph{Political Analysis} 22.4 (2014): 497-519.


\subsection*{National Children's Study consultant}

From 2013 to its ultimate demise in 2014, I was a consultant for the
National Children's Study (NCS), which was being run out of the Eunice
Kennedy Shriver National Institute of Child Health and Human Development
at the National Institutes of Health. Although the NCS was eventually discontinued, 
I nevertheless believe that my three fellow consultants and I made an 
important contribution
to science and were poised to perhaps make a further important contribution
if the NCS had been allowed to continue.

We were asked to assess a sampling design for the NCS where hospitals
were sampled from a list in the first stage and delivering mothers
were sampled from each selected hospital in the second stage. 
We were narrowly asked to evaluate the statistical properties of this approach
using computer simulations.  My role --- to devise and program the simulations in R 
--- was an essential component of the team's recommendations. 

We found that this research design was not particularly efficient, due to the high
degree of socio-demographic clustering of mothers within a hospital (the Primary Sampling Unit). 
Nevertheless, the three other consultants and I agreed that our simulations were the first
time that a survey design with hospitals as PSUs had been rigorously studied.
Without our simulations, the NCS might have been flying blind with a survey design that
would not have served medical researchers well. At the time that the NCS was discontinued, 
we were planning a set of follow-up simulations to better compare alternative survey designs 
using Stan, Census data, and a list of obstetricians that worked in each hospital.

\vspace{2mm}


Frankel, Martin, et al. Final revision of non-public memorandum submitted
to Jennifer Kwan at the National Institutes of Health on 4/1/2014.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                          Research Support                                            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{D. Research Support}

Alfred P. Sloan Foundation, G-2015-13987, \emph{Stan Community and
Continuity}, PI: Andrew Gelman

\noindent Role: Contractor during the Summer of 2015

The overall goal of the Stan project is to make it easier for scientists
to use Bayesian inference in their research. The Sloan Foundation
grant is specifically intended to develop the community of Stan users
and to ensure that it thrives over the long term. My specific role
in this regard was to develop and maintain the R package that provides
an interface to Stan, which is by far the most popular way of using
Stan in the community.

Kennedy Shriver National Institute of Child Health and Human Development
at the National Institutes of Health, \emph{National Children's Study
Sampling Design Consultant}, RFQ 13-048 PO \#275201300316P*1

\noindent Role: Contractor during 2013 and 2014

I --- along with Martin Frankel, Rob Santos, and Nancy Reichman --- 
analyzed a proposed sampling design for the National Children's Study
for Jennifer Kwan and her supervisor Steven Hirschfeld. My role was
primarily to implement the computer simulations to show the properties
of the resulting estimates if the sampling design were implemented on
similar data on births that were provided to us from the State Inpatient 
Database. The research memos we wrote were never made public but were
referred to as NICHD (2013i) in the 2014 report by The Panel on the Design of
the National Children's Study and Implications for the Generalizeability of
the Results.

\end{document}
